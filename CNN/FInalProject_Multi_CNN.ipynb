{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "415d43fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37068 images belonging to 10 classes.\n",
      "Found 18345 images belonging to 10 classes.\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9267/966745523.py:70: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(training_set,\n",
      "2023-05-06 21:24:51.779203: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - loss: 2.2219 - accuracy: 0.1450"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 21:25:02.513262: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 12s 109ms/step - loss: 2.2219 - accuracy: 0.1450 - val_loss: 2.0936 - val_accuracy: 0.2512\n",
      "Epoch 2/40\n",
      "100/100 [==============================] - 12s 115ms/step - loss: 1.9685 - accuracy: 0.2653 - val_loss: 1.6998 - val_accuracy: 0.3769\n",
      "Epoch 3/40\n",
      "100/100 [==============================] - 11s 111ms/step - loss: 1.6712 - accuracy: 0.3609 - val_loss: 1.6610 - val_accuracy: 0.3512\n",
      "Epoch 4/40\n",
      "100/100 [==============================] - 13s 135ms/step - loss: 1.4784 - accuracy: 0.4428 - val_loss: 1.2974 - val_accuracy: 0.5044\n",
      "Epoch 5/40\n",
      "100/100 [==============================] - 17s 168ms/step - loss: 1.3082 - accuracy: 0.5222 - val_loss: 1.0816 - val_accuracy: 0.5925\n",
      "Epoch 6/40\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 1.2897 - accuracy: 0.5222 - val_loss: 1.1633 - val_accuracy: 0.5713\n",
      "Epoch 7/40\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 1.1028 - accuracy: 0.6016 - val_loss: 1.1083 - val_accuracy: 0.5987\n",
      "Epoch 8/40\n",
      "100/100 [==============================] - 20s 199ms/step - loss: 1.0008 - accuracy: 0.6453 - val_loss: 0.9872 - val_accuracy: 0.6388\n",
      "Epoch 9/40\n",
      "100/100 [==============================] - 19s 189ms/step - loss: 0.9636 - accuracy: 0.6597 - val_loss: 0.8216 - val_accuracy: 0.7156\n",
      "Epoch 10/40\n",
      "100/100 [==============================] - 18s 182ms/step - loss: 0.9387 - accuracy: 0.6672 - val_loss: 1.2560 - val_accuracy: 0.5987\n",
      "Epoch 11/40\n",
      "100/100 [==============================] - 26s 262ms/step - loss: 0.8553 - accuracy: 0.7009 - val_loss: 1.0200 - val_accuracy: 0.6562\n",
      "Epoch 12/40\n",
      "100/100 [==============================] - 29s 288ms/step - loss: 0.8091 - accuracy: 0.7316 - val_loss: 0.8988 - val_accuracy: 0.6969\n",
      "Epoch 13/40\n",
      "100/100 [==============================] - 27s 269ms/step - loss: 0.7406 - accuracy: 0.7544 - val_loss: 0.8501 - val_accuracy: 0.6931\n",
      "Epoch 14/40\n",
      "100/100 [==============================] - 27s 274ms/step - loss: 0.7642 - accuracy: 0.7409 - val_loss: 0.8854 - val_accuracy: 0.7131\n",
      "Epoch 15/40\n",
      "100/100 [==============================] - 30s 299ms/step - loss: 0.6883 - accuracy: 0.7692 - val_loss: 0.7928 - val_accuracy: 0.7519\n",
      "Epoch 16/40\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 0.6715 - accuracy: 0.7672 - val_loss: 0.6403 - val_accuracy: 0.7650\n",
      "Epoch 17/40\n",
      "100/100 [==============================] - 21s 214ms/step - loss: 0.6278 - accuracy: 0.7803 - val_loss: 0.9650 - val_accuracy: 0.6956\n",
      "Epoch 18/40\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 0.6701 - accuracy: 0.7775 - val_loss: 0.4488 - val_accuracy: 0.8519\n",
      "Epoch 19/40\n",
      "100/100 [==============================] - 17s 171ms/step - loss: 0.5936 - accuracy: 0.7959 - val_loss: 0.5564 - val_accuracy: 0.7994\n",
      "Epoch 20/40\n",
      "100/100 [==============================] - 17s 171ms/step - loss: 0.6049 - accuracy: 0.8019 - val_loss: 0.4549 - val_accuracy: 0.8537\n",
      "Epoch 21/40\n",
      "100/100 [==============================] - 13s 124ms/step - loss: 0.5817 - accuracy: 0.8022 - val_loss: 0.4411 - val_accuracy: 0.8631\n",
      "Epoch 22/40\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 0.5514 - accuracy: 0.8116 - val_loss: 0.6596 - val_accuracy: 0.7869\n",
      "Epoch 23/40\n",
      "100/100 [==============================] - 11s 106ms/step - loss: 0.5538 - accuracy: 0.8172 - val_loss: 0.7760 - val_accuracy: 0.7506\n",
      "Epoch 24/40\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 0.5102 - accuracy: 0.8334 - val_loss: 0.8209 - val_accuracy: 0.7556\n",
      "Epoch 25/40\n",
      "100/100 [==============================] - 11s 112ms/step - loss: 0.5911 - accuracy: 0.8078 - val_loss: 0.4807 - val_accuracy: 0.8369\n",
      "Epoch 26/40\n",
      "100/100 [==============================] - 11s 113ms/step - loss: 0.5469 - accuracy: 0.8128 - val_loss: 0.6891 - val_accuracy: 0.7744\n",
      "Epoch 27/40\n",
      "100/100 [==============================] - 11s 113ms/step - loss: 0.4724 - accuracy: 0.8400 - val_loss: 0.3952 - val_accuracy: 0.8719\n",
      "Epoch 28/40\n",
      "100/100 [==============================] - 11s 107ms/step - loss: 0.4896 - accuracy: 0.8391 - val_loss: 0.4602 - val_accuracy: 0.8512\n",
      "Epoch 29/40\n",
      "100/100 [==============================] - 11s 110ms/step - loss: 0.5234 - accuracy: 0.8156 - val_loss: 0.4979 - val_accuracy: 0.8294\n",
      "Epoch 30/40\n",
      "100/100 [==============================] - 11s 108ms/step - loss: 0.5292 - accuracy: 0.8184 - val_loss: 0.3730 - val_accuracy: 0.8781\n",
      "Epoch 31/40\n",
      "100/100 [==============================] - 11s 108ms/step - loss: 0.4811 - accuracy: 0.8456 - val_loss: 0.4673 - val_accuracy: 0.8537\n",
      "Epoch 32/40\n",
      "100/100 [==============================] - 11s 112ms/step - loss: 0.4464 - accuracy: 0.8547 - val_loss: 0.6744 - val_accuracy: 0.7800\n",
      "Epoch 33/40\n",
      "100/100 [==============================] - 11s 107ms/step - loss: 0.4434 - accuracy: 0.8556 - val_loss: 0.4286 - val_accuracy: 0.8506\n",
      "Epoch 34/40\n",
      "100/100 [==============================] - 11s 107ms/step - loss: 0.4444 - accuracy: 0.8466 - val_loss: 0.7203 - val_accuracy: 0.7713\n",
      "Epoch 35/40\n",
      "100/100 [==============================] - 11s 111ms/step - loss: 0.4364 - accuracy: 0.8509 - val_loss: 0.4267 - val_accuracy: 0.8475\n",
      "Epoch 36/40\n",
      "100/100 [==============================] - 11s 106ms/step - loss: 0.4545 - accuracy: 0.8478 - val_loss: 0.5439 - val_accuracy: 0.8263\n",
      "Epoch 37/40\n",
      "100/100 [==============================] - 11s 108ms/step - loss: 0.4133 - accuracy: 0.8637 - val_loss: 0.5021 - val_accuracy: 0.8300\n",
      "Epoch 38/40\n",
      "100/100 [==============================] - 11s 108ms/step - loss: 0.3801 - accuracy: 0.8759 - val_loss: 0.5423 - val_accuracy: 0.8269\n",
      "Epoch 39/40\n",
      "100/100 [==============================] - 11s 111ms/step - loss: 0.4194 - accuracy: 0.8622 - val_loss: 0.4599 - val_accuracy: 0.8487\n",
      "Epoch 40/40\n",
      "100/100 [==============================] - 11s 110ms/step - loss: 0.4606 - accuracy: 0.8469 - val_loss: 0.3415 - val_accuracy: 0.8869\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/Single/J.JPG'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 79\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# prediction of the image\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m image\n\u001b[0;32m---> 79\u001b[0m test_image \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_img\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mData/Single/J.JPG\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m test_image\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     81\u001b[0m test_image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mimg_to_array (test_image)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/image_utils.py:422\u001b[0m, in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, pathlib\u001b[38;5;241m.\u001b[39mPath):\n\u001b[1;32m    421\u001b[0m         path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(path\u001b[38;5;241m.\u001b[39mresolve())\n\u001b[0;32m--> 422\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    423\u001b[0m         img \u001b[38;5;241m=\u001b[39m pil_image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(f\u001b[38;5;241m.\u001b[39mread()))\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/Single/J.JPG'"
     ]
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dropout\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from PIL import Image\n",
    "\n",
    "# Initialising the CNN\n",
    "model = Sequential();\n",
    "\n",
    "#input layer\n",
    "#Convolution\n",
    "model.add(Conv2D(32,(3,3),input_shape=(64,64,3), activation='relu'))\n",
    "#Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#second layer\n",
    "model.add(Conv2D(32,(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#third layer\n",
    "model.add(Conv2D(16,(3,3),input_shape=(64,64,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#Flattening before going into dense layer\n",
    "model.add(Flatten())\n",
    "\n",
    "#Full connection\n",
    "model.add(Dense(2000, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#another dense layer\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#another dense layer\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#another dense layer\n",
    "model.add(Dense(250, activation='relu'))\n",
    "\n",
    "#Adding the Output Layer\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "#Compiling the CNN\n",
    "model.compile(optimizer ='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('Multi_Data/Train',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('Multi_Data/Test',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'categorical')\n",
    "\n",
    "model.fit_generator(training_set,\n",
    "                         steps_per_epoch = 100,\n",
    "                         epochs = 40,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 50)\n",
    "\n",
    "\n",
    "# prediction of the image\n",
    "from tensorflow.keras.preprocessing import image\n",
    "test_image = image.load_img ('Data/Single/J.JPG', target_size= (64, 64))\n",
    "test_image.show()\n",
    "test_image = image.img_to_array (test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "results = model.predict(test_image)\n",
    "training_set.class_indices\n",
    "if results[0][0] == 1:\n",
    "    prediction = 'A'\n",
    "    print(prediction)\n",
    "elif results[0][1] == 2:\n",
    "    prediction = 'B'\n",
    "    print(prediction)\n",
    "elif results[0][2] == 3:\n",
    "    prediction = 'C'\n",
    "    print(prediction)\n",
    "elif results[0][3] == 4:\n",
    "    prediction = 'D'\n",
    "    print(prediction)\n",
    "elif results[0][4] == 5:\n",
    "    prediction = 'E'\n",
    "    print(prediction)\n",
    "elif results[0][5] == 6:\n",
    "    prediction = 'F'\n",
    "    print(prediction)\n",
    "elif results[0][6] == 7:\n",
    "    prediction = 'G'\n",
    "    print(prediction)\n",
    "elif results[0][7] == 8:\n",
    "    prediction = 'H'\n",
    "    print(prediction)\n",
    "elif results[0][8] == 9:\n",
    "    prediction = 'I'\n",
    "    print(prediction)\n",
    "else:\n",
    "    prediction = 'J'\n",
    "    print(prediction)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e68ecd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle \n",
    "\n",
    "# # Save the trained model as a pickle string. \n",
    "\n",
    "# with open ('model','wb') as f:\n",
    "\n",
    "#      pickle.dump(model,f)   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0459308",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('multi_CNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d0c9aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 15:09:15.220216: W tensorflow/c/c_api.cc:300] Operation '{name:'dense_2/bias/Assign' id:169 op device:{requested: '', assigned: ''} def:{{{node dense_2/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_2/bias, dense_2/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 15:09:15.981470: W tensorflow/c/c_api.cc:300] Operation '{name:'dense_4/Softmax' id:232 op device:{requested: '', assigned: ''} def:{{{node dense_4/Softmax}} = Softmax[T=DT_FLOAT, _has_manual_control_dependencies=true](dense_4/BiasAdd)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-05-06 15:09:16.023707: W tensorflow/c/c_api.cc:300] Operation '{name:'total/Assign' id:311 op device:{requested: '', assigned: ''} def:{{{node total/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](total, total/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import Graph\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "model_graph = Graph()\n",
    "\n",
    "\n",
    "with model_graph.as_default():\n",
    "    tf_session = tf.compat.v1.Session()\n",
    "    with tf_session.as_default():\n",
    "        model1=load_model('multi_CNN.h5')\n",
    "#         model2 = load_model('./models/multi_model.h5')\n",
    "\n",
    "\n",
    "img = image.load_img(\"Data/Single/E.JPG\", target_size=(64, 64))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "with model_graph.as_default():\n",
    "    with tf_session.as_default():\n",
    "        prediction = model1.predict(x, batch_size=10)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b1b93e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 21:23:22.268040: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-06 21:23:22.315705: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-06 21:23:22.316407: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-06 21:23:23.165315: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "No file or directory found at multi_CNN.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m     tf_session \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mSession()\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf_session\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[0;32m---> 15\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmulti_CNN.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m image\n\u001b[1;32m     18\u001b[0m test_image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mload_img (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData/Single/A.JPG\u001b[39m\u001b[38;5;124m'\u001b[39m, target_size\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/saving/saving_api.py:212\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    205\u001b[0m         filepath,\n\u001b[1;32m    206\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[1;32m    208\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[1;32m    209\u001b[0m     )\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_sm_saving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/saving/legacy/save.py:230\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(filepath_str):\n\u001b[0;32m--> 230\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[1;32m    231\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    232\u001b[0m         )\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(\n\u001b[1;32m    236\u001b[0m             filepath_str, \u001b[38;5;28mcompile\u001b[39m, options\n\u001b[1;32m    237\u001b[0m         )\n",
      "\u001b[0;31mOSError\u001b[0m: No file or directory found at multi_CNN.h5"
     ]
    }
   ],
   "source": [
    "from tensorflow import Graph\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_graph = Graph()\n",
    "\n",
    "with model_graph.as_default():\n",
    "    tf_session = tf.compat.v1.Session()\n",
    "    with tf_session.as_default():\n",
    "        model=load_model('multi_CNN.h5')\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "test_image = image.load_img ('Data/Single/A.JPG', target_size= (64, 64))\n",
    "test_image.show()\n",
    "test_image = image.img_to_array (test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "\n",
    "with model_graph.as_default():\n",
    "    with tf_session.as_default():\n",
    "        results = model.predict(test_image, batch_size=10)\n",
    "\n",
    "# results = model.predict(test_image)\n",
    "class_labels = training_set.class_indices\n",
    "if results[0][0] == np.max(results):\n",
    "    prediction = list(class_labels.keys())[list(class_labels.values()).index(0)]\n",
    "elif results[0][1] == np.max(results):\n",
    "    prediction = list(class_labels.keys())[list(class_labels.values()).index(1)]\n",
    "elif results[0][2] == np.max(results):\n",
    "    prediction = list(class_labels.keys())[list(class_labels.values()).index(2)]\n",
    "elif results[0][3] == np.max(results):\n",
    "    prediction = list(class_labels.keys())[list(class_labels.values()).index(3)]\n",
    "elif results[0][4] == np.max(results):\n",
    "    prediction = list(class_labels.keys())[list(class_labels.values()).index(4)]\n",
    "elif results[0][5] == np.max(results):\n",
    "    prediction = list(class_labels.keys())[list(class_labels.values()).index(5)]\n",
    "elif results[0][6] == np.max(results):\n",
    "    prediction = list(class_labels.keys())[list(class_labels.values()).index(6)]\n",
    "elif results[0][7] == np.max(results):\n",
    "    prediction = list(class_labels.keys())[list(class_labels.values()).index(7)]\n",
    "elif results[0][8] == np.max(results):\n",
    "    prediction = list(class_labels.keys())[list(class_labels.values()).index(8)]    \n",
    "else:\n",
    "    prediction = list(class_labels.keys())[list(class_labels.values()).index(9)]\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f7db73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
