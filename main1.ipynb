{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceb57107",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [0, 300]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 456\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# Training and Testing Data (divide the data into two part)\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m--> 456\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m\u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m    459\u001b[0m sc \u001b[38;5;241m=\u001b[39m StandardScaler()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2445\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2445\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2447\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   2448\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2449\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2450\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:433\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    415\u001b[0m \n\u001b[0;32m    416\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    432\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 433\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:387\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    385\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 387\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    390\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [0, 300]"
     ]
    }
   ],
   "source": [
    "import xlsxwriter\n",
    "import openpyxl\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import math\n",
    "import statistics\n",
    "import cv2\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib \n",
    "from matplotlib import image\n",
    "from matplotlib import pyplot\n",
    "from numpy import asarray\n",
    "import scipy\n",
    "from scipy.stats import skew, kurtosis\n",
    "from skimage import io\n",
    "import skimage.measure    \n",
    "import skimage.feature\n",
    "import xlsxwriter\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from PIL import Image\n",
    " \n",
    "Image_list = []\n",
    "\n",
    "# Create File\n",
    "outWorkbook = xlsxwriter.Workbook(\"out.xlsx\")\n",
    "outSheet = outWorkbook.add_worksheet()\n",
    "\n",
    "# Write Headers\n",
    "outSheet.write(\"A1\", \"Mean1\")\n",
    "outSheet.write(\"B1\", \"Variance1\")\n",
    "outSheet.write(\"C1\", \"Skewness1\")\n",
    "outSheet.write(\"D1\", \"Kurtosis1\")\n",
    "outSheet.write(\"E1\", \"Contrast1\")\n",
    "outSheet.write(\"F1\", \"Entropy1\")\n",
    "outSheet.write(\"G1\", \"Energy1\")\n",
    "outSheet.write(\"H1\", \"Homo1\")\n",
    "outSheet.write(\"I1\", \"Corre1\")\n",
    "outSheet.write(\"J1\", \"Mean2\")\n",
    "outSheet.write(\"K1\", \"Variance2\")\n",
    "outSheet.write(\"L1\", \"Skewness2\")\n",
    "outSheet.write(\"M1\", \"Kurtosis2\")\n",
    "outSheet.write(\"N1\", \"Contrast2\")\n",
    "outSheet.write(\"O1\", \"Entropy2\")\n",
    "outSheet.write(\"P1\", \"Energy2\")\n",
    "outSheet.write(\"Q1\", \"Homo2\")\n",
    "outSheet.write(\"R1\", \"Corre2\")\n",
    "outSheet.write(\"S1\", \"Mean3\")\n",
    "outSheet.write(\"T1\", \"Variance3\")\n",
    "outSheet.write(\"U1\", \"Skewness3\")\n",
    "outSheet.write(\"V1\", \"Kurtosis3\")\n",
    "outSheet.write(\"W1\", \"Contrast3\")\n",
    "outSheet.write(\"X1\", \"Entropy3\")\n",
    "outSheet.write(\"Y1\", \"Energy3\")\n",
    "outSheet.write(\"Z1\", \"Homo13\")\n",
    "outSheet.write(\"AA1\", \"Corre3\")\n",
    "outWorkbook.close()\n",
    "\n",
    "\n",
    "# Define input and output directories\n",
    "input_dir = 'C:/Users/arfan.shah/Desktop/UCA_Undergrad/Arfan_FinalYearProject/data1/*.*'\n",
    "output_dir = 'C:/Users/arfan.shah/Desktop/UCA_Undergrad/Arfan_FinalYearProject/'\n",
    "\n",
    "# Create output directory if it does not exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Loop over all JPEG files in the input directory\n",
    "for filename in input_dir:\n",
    "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "        # Load the image\n",
    "        img = cv2.imread(os.path.join(input_dir, filename))\n",
    "        \n",
    "        # Split the image into R,G,B channels\n",
    "        blue, green, red = cv2.split(img)\n",
    "        \n",
    "        # Apply PCA to the red channel\n",
    "        pca = PCA().fit(red)\n",
    "        var = np.cumsum(pca.explained_variance_ratio_)\n",
    "        n_components_red = np.argmax(var >= 0.95) + 1\n",
    "        \n",
    "        # Apply PCA to the green channel\n",
    "        pca = PCA().fit(green)\n",
    "        var = np.cumsum(pca.explained_variance_ratio_)\n",
    "        n_components_green = np.argmax(var >= 0.95) + 1\n",
    "        \n",
    "        # Apply PCA to the blue channel\n",
    "        pca = PCA().fit(blue)\n",
    "        var = np.cumsum(pca.explained_variance_ratio_)\n",
    "        n_components_blue = np.argmax(var >= 0.95) + 1\n",
    "        \n",
    "        # Initialize PCA with the selected number of components\n",
    "        n_components = min(n_components_red, n_components_green, n_components_blue)\n",
    "        pca = PCA(n_components)\n",
    "        \n",
    "        # Apply PCA to the red channel and then apply inverse transform to transformed array\n",
    "        red_transformed = pca.fit_transform(red)\n",
    "        red_inverted = pca.inverse_transform(red_transformed)\n",
    "        \n",
    "        # Apply PCA to the green channel and then apply inverse transform to transformed array\n",
    "        green_transformed = pca.fit_transform(green)\n",
    "        green_inverted = pca.inverse_transform(green_transformed)\n",
    "        \n",
    "        # Apply PCA to the blue channel and then apply inverse transform to transformed array\n",
    "        blue_transformed = pca.fit_transform(blue)\n",
    "        blue_inverted = pca.inverse_transform(blue_transformed)\n",
    "\n",
    "        # Combine the color channels into a compressed image\n",
    "        img_compressed = (np.dstack((blue_inverted, green_inverted, red_inverted))).astype(np.uint8)\n",
    "         \n",
    "        image = np.array(img_compressed)\n",
    "        Image_list.append(image) \n",
    " \n",
    "for i in range(0, len(Image_list)):\n",
    "    Image = Image_list[i]  \n",
    "    image = asarray(Image) \n",
    "    R = image[:,:,0]\n",
    "    G = image[:,:,1]\n",
    "    B = image[:,:,2] \n",
    "     \n",
    "    # Histogram Equalization for Red Channel\n",
    "    R = asarray(R) \n",
    "    # put pixels in a 1D array by flattening out img array\n",
    "    flat = R.flatten()\n",
    "    # Histogram Equalization\n",
    "    # show the histogram\n",
    "    pyplot.figure(1)\n",
    "    pyplot.hist(flat, bins=10)\n",
    "\n",
    "    # create our own histogram function\n",
    "    def get_histogram1(R, bins):\n",
    "        # array with size of bins, set to zeros\n",
    "        histogram1 = np.zeros(bins)\n",
    "# loop through pixels and sum up counts of pixels\n",
    "        for pixel in R:\n",
    "            histogram1[pixel] += 1\n",
    "    \n",
    "# return our final result\n",
    "        return histogram1\n",
    "# execute our histogram function\n",
    "    hist = get_histogram1(flat, 256)\n",
    "    # create our cumulative sum function\n",
    "    def cumsum(a1):\n",
    "        a1 = iter(a1)\n",
    "        b1 = [next(a1)]\n",
    "        for i in a1:\n",
    "            b1.append(b1[-1] + i)\n",
    "        return np.array(b1)\n",
    "# execute the fn\n",
    "    cs1 = cumsum(hist)\n",
    "\n",
    "# display the result\n",
    "    pyplot.figure(2)\n",
    "    pyplot.plot(cs1)\n",
    "\n",
    "# numerator & denomenator\n",
    "    nj1 = (cs1 - cs1.min()) * 255\n",
    "    N1 = cs1.max() - cs1.min()\n",
    "\n",
    "# re-normalize the cumsum\n",
    "    cs1 = nj1 / N1\n",
    "\n",
    "# cast it back to uint8 since we can't use floating point values in images\n",
    "# pyplot.figure(1)\n",
    "    cs1 = cs1.astype('uint8')\n",
    "\n",
    "    pyplot.figure(3)\n",
    "    pyplot.plot(cs1)\n",
    "\n",
    "# get the value from cumulative sum for every index in flat, and set that as img_new\n",
    "    img_new1 = cs1[flat]\n",
    "\n",
    "# put array back into original shape since we flattened it\n",
    "    img_new1 = np.reshape(img_new1, R.shape)\n",
    "\n",
    "# set up side-by-side image display\n",
    "# fig = pyplot.figure(3)\n",
    "# fig.set_figheight(15)\n",
    "# fig.set_figwidth(15)\n",
    "\n",
    "# fig.add_subplot(1,2,1)\n",
    "# pyplot.figure(4)\n",
    "# pyplot.imshow(R, cmap='gray')\n",
    "\n",
    "# display the new image\n",
    "# fig.add_subplot(1,2,2)\n",
    "    pyplot.figure(4)\n",
    "    pyplot.imshow(img_new1, cmap='gray')\n",
    "\n",
    "# pyplot.show(block=True)\n",
    "\n",
    "    R = img_new1\n",
    "\n",
    "\n",
    "  # Histogram Equalization for Red Channel\n",
    "    G = asarray(G) \n",
    "    # put pixels in a 1D array by flattening out img array\n",
    "    flat = G.flatten()\n",
    "    # Histogram Equalization\n",
    "    # show the histogram\n",
    "    pyplot.figure(1)\n",
    "    pyplot.hist(flat, bins=10)\n",
    "\n",
    "    # create our own histogram function\n",
    "    def get_histogram2(G, bins):\n",
    "        # array with size of bins, set to zeros\n",
    "        histogram2 = np.zeros(bins)\n",
    "# loop through pixels and sum up counts of pixels\n",
    "        for pixel in G:\n",
    "            histogram2[pixel] += 1\n",
    "    \n",
    "# return our final result\n",
    "        return histogram2\n",
    "# execute our histogram function\n",
    "    hist = get_histogram2(flat, 256)\n",
    "    # create our cumulative sum function\n",
    "    def cumsum(a2):\n",
    "        a2 = iter(a2)\n",
    "        b2 = [next(a2)]\n",
    "        for i in a2:\n",
    "            b2.append(b2[-1] + i)\n",
    "        return np.array(b2)\n",
    "# execute the fn\n",
    "    cs2 = cumsum(hist)\n",
    "\n",
    "# display the result\n",
    "    pyplot.figure(2)\n",
    "    pyplot.plot(cs2)\n",
    "\n",
    "# numerator & denomenator\n",
    "    nj2 = (cs2 - cs2.min()) * 255\n",
    "    N2 = cs2.max() - cs2.min()\n",
    "\n",
    "# re-normalize the cumsum\n",
    "    cs2 = nj2 / N2\n",
    "\n",
    "# cast it back to uint8 since we can't use floating point values in images\n",
    "# pyplot.figure(1)\n",
    "    cs2 = cs2.astype('uint8')\n",
    "\n",
    "    pyplot.figure(3)\n",
    "    pyplot.plot(cs2)\n",
    "\n",
    "# get the value from cumulative sum for every index in flat, and set that as img_new\n",
    "    img_new2 = cs2[flat]\n",
    "\n",
    "# put array back into original shape since we flattened it\n",
    "    img_new2 = np.reshape(img_new2, G.shape)\n",
    "\n",
    "# set up side-by-side image display\n",
    "# fig = pyplot.figure(3)\n",
    "# fig.set_figheight(15)\n",
    "# fig.set_figwidth(15)\n",
    "\n",
    "# fig.add_subplot(1,2,1)\n",
    "# pyplot.figure(4)\n",
    "# pyplot.imshow(R, cmap='gray')\n",
    "\n",
    "# display the new image\n",
    "# fig.add_subplot(1,2,2)\n",
    "    pyplot.figure(4)\n",
    "    pyplot.imshow(img_new2, cmap='gray')\n",
    "\n",
    "# pyplot.show(block=True)\n",
    "\n",
    "    G = img_new2\n",
    "    \n",
    "    \n",
    "      \n",
    "    # Histogram Equalization for Red Channel\n",
    "    B = asarray(B) \n",
    "    # put pixels in a 1D array by flattening out img array\n",
    "    flat = B.flatten()\n",
    "    # Histogram Equalization\n",
    "    # show the histogram\n",
    "    pyplot.figure(1)\n",
    "    pyplot.hist(flat, bins=10)\n",
    "\n",
    "    # create our own histogram function\n",
    "    def get_histogram3(B, bins):\n",
    "        # array with size of bins, set to zeros\n",
    "        histogram3 = np.zeros(bins)\n",
    "# loop through pixels and sum up counts of pixels\n",
    "        for pixel in B:\n",
    "            histogram3[pixel] += 1\n",
    "    \n",
    "# return our final result\n",
    "        return histogram3\n",
    "# execute our histogram function\n",
    "    hist = get_histogram3(flat, 256)\n",
    "    # create our cumulative sum function\n",
    "    def cumsum(a3):\n",
    "        a3 = iter(a3)\n",
    "        b3 = [next(a3)]\n",
    "        for i in a3:\n",
    "            b3.append(b3[-1] + i)\n",
    "        return np.array(b3)\n",
    "# execute the fn\n",
    "    cs3 = cumsum(hist)\n",
    "\n",
    "# display the result\n",
    "    pyplot.figure(3)\n",
    "    pyplot.plot(cs3)\n",
    "\n",
    "# numerator & denomenator\n",
    "    nj3 = (cs3 - cs3.min()) * 255\n",
    "    N3 = cs3.max() - cs3.min()\n",
    "\n",
    "# re-normalize the cumsum\n",
    "    cs3 = nj3 / N3\n",
    "\n",
    "# cast it back to uint8 since we can't use floating point values in images\n",
    "# pyplot.figure(1)\n",
    "    cs3 = cs3.astype('uint8')\n",
    "\n",
    "    pyplot.figure(3)\n",
    "    pyplot.plot(cs3)\n",
    "\n",
    "# get the value from cumulative sum for every index in flat, and set that as img_new\n",
    "    img_new3 = cs3[flat]\n",
    "\n",
    "# put array back into original shape since we flattened it\n",
    "    img_new3 = np.reshape(img_new3, B.shape)\n",
    "\n",
    "# set up side-by-side image display\n",
    "# fig = pyplot.figure(3)\n",
    "# fig.set_figheight(15)\n",
    "# fig.set_figwidth(15)\n",
    "\n",
    "# fig.add_subplot(1,2,1)\n",
    "# pyplot.figure(4)\n",
    "# pyplot.imshow(R, cmap='gray')\n",
    "\n",
    "# display the new image\n",
    "# fig.add_subplot(1,2,2)\n",
    "    pyplot.figure(4)\n",
    "    pyplot.imshow(img_new3, cmap='gray')\n",
    "\n",
    "# pyplot.show(block=True)\n",
    "\n",
    "    B = img_new3\n",
    "    \n",
    "       \n",
    "    # Feature Selection from Red Channel\n",
    "    print (\"Calucation nine Statistical features for Red Channel\")\n",
    "    Mean1 = np.mean(R);\n",
    "    print (Mean1)\n",
    "    Variance1 = np.var(R)\n",
    "    Variance1 = math.sqrt(Variance1)\n",
    "    print(Variance1)\n",
    "    Skewness1=skew(R.reshape(-1))\n",
    "    print (Skewness1)\n",
    "    Kurtosis1=kurtosis(R.reshape(-1))\n",
    "    print (Kurtosis1)\n",
    "    entropy1 = skimage.measure.shannon_entropy(R)\n",
    "    print (entropy1)\n",
    "    R = skimage.img_as_ubyte(R)\n",
    "    g1 = skimage.feature.greycomatrix(R, [1], [0], levels=256, symmetric=False, normed=True)\n",
    "    Cont1 =skimage.feature.greycoprops(g1, 'contrast')[0][0]\n",
    "    print (Cont1)\n",
    "    Energ1 =skimage.feature.greycoprops(g1, 'energy')[0][0]\n",
    "    print (Energ1)\n",
    "    Homo1=skimage.feature.greycoprops(g1, 'homogeneity')[0][0]\n",
    "    print (Homo1)\n",
    "    Corre1=skimage.feature.greycoprops(g1, 'correlation')[0][0]\n",
    "    print (Corre1)\n",
    "    \n",
    "    \n",
    "    # Feature Selection from Green Channel\n",
    "    print (\"Calculate Statistical Features for Green Channel\")\n",
    "    Mean2 = np.mean(G);\n",
    "    print (Mean2)\n",
    "    Variance2 = np.var(G)\n",
    "    Variance2 = math.sqrt(Variance2)\n",
    "    print(Variance2)\n",
    "    Skewness2=skew(G.reshape(-1))\n",
    "    print (Skewness2)\n",
    "    Kurtosis2=kurtosis(G.reshape(-1))\n",
    "    print (Kurtosis2)\n",
    "    entropy2 = skimage.measure.shannon_entropy(G)\n",
    "    print (entropy2)\n",
    "    G = skimage.img_as_ubyte(G)\n",
    "    g2 = skimage.feature.greycomatrix(G, [1], [0], levels=256, symmetric=False, normed=True)\n",
    "    Cont2 =skimage.feature.greycoprops(g2, 'contrast')[0][0]\n",
    "    print (Cont2)\n",
    "    Energ2 =skimage.feature.greycoprops(g2, 'energy')[0][0]\n",
    "    print (Energ2)\n",
    "    Homo2=skimage.feature.greycoprops(g2, 'homogeneity')[0][0]\n",
    "    print (Homo2)\n",
    "    Corre2=skimage.feature.greycoprops(g2, 'correlation')[0][0]\n",
    "    print (Corre2)\n",
    "    \n",
    "    #  Feature Selection from Blue Channel\n",
    "    print (\"Calucation  Statistical Features for Blue Channel\")\n",
    "    Mean3 = np.mean(B);\n",
    "    print (Mean3)\n",
    "    Variance3 = np.var(B)\n",
    "    Variance3 = math.sqrt(Variance3)\n",
    "    print(Variance3)\n",
    "    Skewness3=skew(B.reshape(-1))\n",
    "    print (Skewness3)\n",
    "    Kurtosis3=kurtosis(B.reshape(-1))\n",
    "    print (Kurtosis3)\n",
    "    entropy3 = skimage.measure.shannon_entropy(B)\n",
    "    print (entropy3)\n",
    "    B = skimage.img_as_ubyte(B)\n",
    "    g3 = skimage.feature.greycomatrix(B, [1], [0], levels=256, symmetric=False, normed=True)\n",
    "    Cont3 =skimage.feature.greycoprops(g3, 'contrast')[0][0]\n",
    "    print (Cont3)\n",
    "    Energ3 =skimage.feature.greycoprops(g3, 'energy')[0][0]\n",
    "    print (Energ3)\n",
    "    Homo3=skimage.feature.greycoprops(g3, 'homogeneity')[0][0]\n",
    "    print (Homo3)\n",
    "    Corre3=skimage.feature.greycoprops(g3, 'correlation')[0][0]\n",
    "    print (Corre3)\n",
    "    \n",
    "\n",
    "    print(\"How many Times\" )\n",
    "    print (i)\n",
    "    \n",
    "    # Create File\n",
    "    outWorkbook = xlsxwriter.Workbook(\"out.xlsx\")\n",
    "    outSheet = outWorkbook.add_worksheet()\n",
    "    \n",
    "    # Declare Data\n",
    "    values = [Mean1, Variance1, Skewness1, Kurtosis1,entropy1, Cont1,Energ1, Homo1, Corre1, Mean2, Variance2, Skewness2, Kurtosis2, entropy2, Cont2, Energ2, Homo2, Corre2, Mean3, Variance3, Skewness3, Kurtosis3, entropy3, Cont3, Energ3, Homo3, Corre3 ]\n",
    "    \n",
    "     \n",
    "    # Write data to file\n",
    "    outWorkbook = openpyxl.load_workbook(\"out.xlsx\") \n",
    "    outSheet = outWorkbook.active\n",
    "    outSheet.append(values)\n",
    "\n",
    "    outWorkbook.save(filename=\"out.xlsx\")\n",
    "\n",
    "df = pd.read_excel(\"out.xlsx\")\n",
    "df.to_csv(\"data.csv\")\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('data.csv')\n",
    "dataset1 = pd.read_csv('Labels1.csv')\n",
    "X = dataset.iloc[:, 0:-1].values\n",
    "y = dataset1.iloc[:, -1].values\n",
    "    \n",
    "# Training and Testing Data (divide the data into two part)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test =train_test_split(X,y,test_size=0.25, random_state=0)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier=DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "y_pred= classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,y_pred)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "cm = confusion_matrix(y_pred, y_test)\n",
    "sns.heatmap(cm,annot=True)\n",
    "pyplot.savefig('h.png')\n",
    "print(cm)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,y_pred)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "import pickle \n",
    "  \n",
    "# Save the trained model as a pickle string. \n",
    "with open ('classifier_pickle','wb') as f:\n",
    "    pickle.dump(classifier,f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5193222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
